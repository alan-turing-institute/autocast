{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## AutoCast encoder-processor-decoder model API Exploration\n",
    "\n",
    "This notebook aims to explore the end-to-end API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Example dataaset\n",
    "\n",
    "We use the `AdvectionDiffusion` dataset as an example dataset to illustrate training and evaluation of models. This dataset simulates the advection-diffusion equation in 2D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mfamili/work/auto-cast/.venv/lib/python3.12/site-packages/pyro/ops/stats.py:514: SyntaxWarning: invalid escape sequence '\\g'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from autoemulate.simulations.advection_diffusion import AdvectionDiffusion as Sim\n",
    "\n",
    "sim = Sim(return_timeseries=True, log_level=\"error\")\n",
    "\n",
    "\n",
    "def generate_split(simulator: Sim, n_train: int = 10, n_valid: int = 2, n_test: int = 2):\n",
    "    \"\"\"Generate training, validation, and test splits from the simulator.\"\"\"\n",
    "    train = simulator.forward_samples_spatiotemporal(n_train)\n",
    "    valid = simulator.forward_samples_spatiotemporal(n_valid)\n",
    "    test = simulator.forward_samples_spatiotemporal(n_test)\n",
    "    return {\"train\": train, \"valid\": valid, \"test\": test}\n",
    "\n",
    "\n",
    "combined_data = generate_split(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Read combined data into datamodule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_cast.data.datamodule import SpatioTemporalDataModule\n",
    "\n",
    "n_steps_input = 1\n",
    "n_steps_output = 4\n",
    "datamodule = SpatioTemporalDataModule(\n",
    "    data=combined_data,\n",
    "    data_path=None,\n",
    "    n_steps_input=n_steps_input,\n",
    "    n_steps_output=n_steps_output,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Example batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 1, 50, 50, 1]), torch.Size([16, 4, 50, 50, 1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "\n",
    "batch.input_fields.shape, batch.output_fields.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57d2b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "from auto_cast.encoders.base import Encoder\n",
    "from auto_cast.types import Batch, Tensor, TensorBCWH\n",
    "\n",
    "\n",
    "class IdentityEncoder(Encoder):\n",
    "    \"\"\"Permute and concatenate Encoder.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    def forward(self, batch: Batch) -> Tensor:\n",
    "        return batch.input_fields\n",
    "\n",
    "    def encode(self, batch: Batch) -> TensorBCWH:\n",
    "        return self.forward(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30092261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "from auto_cast.decoders.base import Decoder\n",
    "from auto_cast.types import TensorBCTSPlus, TensorBMStarL, TensorBTSPlusC\n",
    "\n",
    "\n",
    "class IdentityDecoder(Decoder):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    def forward(self, x: TensorBCTSPlus) -> TensorBTSPlusC:\n",
    "        return x\n",
    "\n",
    "    def decode(self, z: TensorBTSPlusC) -> TensorBTSPlusC:\n",
    "        return self.forward(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debaa9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from azula.nn.unet import UNet\n",
    "from azula.nn.embedding import SineEncoding\n",
    "\n",
    "class TemporalUNetBackbone(nn.Module):\n",
    "    \"\"\"Azula UNet with proper time embedding.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 1,\n",
    "        out_channels: int = 1,\n",
    "        cond_channels: int = 1,\n",
    "        mod_features: int = 256,\n",
    "        hid_channels: tuple = (32, 64, 128),\n",
    "        hid_blocks: tuple = (2, 2, 2),\n",
    "        spatial: int = 2,\n",
    "        periodic: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            SineEncoding(mod_features),\n",
    "            nn.Linear(mod_features, mod_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(mod_features, mod_features),\n",
    "        )\n",
    "        \n",
    "        self.unet = UNet(\n",
    "            in_channels=in_channels + cond_channels,\n",
    "            out_channels=out_channels,\n",
    "            cond_channels=0,\n",
    "            mod_features=mod_features,\n",
    "            hid_channels=hid_channels,\n",
    "            hid_blocks=hid_blocks,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            spatial=spatial,\n",
    "            periodic=periodic,\n",
    "        )\n",
    "\n",
    "    def forward(self, x_out: torch.Tensor, t: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_out: Noisy data (B, T, C, H, W) - channels first from Azula\n",
    "            t: Time steps (B,)\n",
    "            cond: Conditioning input (B, T_cond, C, H, W) - channels first\n",
    "        Returns:\n",
    "            Denoised output (B, T, C, H, W)\n",
    "        \"\"\"\n",
    "        B, T, W, H, C = x_out.shape\n",
    "        _, T_cond, W_cond, H_cond , C_cond = cond.shape\n",
    "        assert W == W_cond and H == H_cond\n",
    "        print(\"x_out.shape\", x_out.shape)\n",
    "        print(\"cond.shape\", cond.shape)\n",
    "        # Embed time (once per batch)\n",
    "        t_emb = self.time_embedding(t)  # (B, mod_features)\n",
    "        mod_for_unet = t_emb\n",
    "        print(t_emb.shape)\n",
    "        t_emb = rearrange(t_emb, \"b m -> b  1 1 1 m\")\n",
    "        t_emb = t_emb.expand(B, T_cond, W, H, -1)  # (B, mod_features, H, W)\n",
    "\n",
    "        print(\"t_emb.shape\", t_emb.shape)\n",
    "        # Concatenate along channel dimension\n",
    "        x_cond = torch.cat([cond, t_emb], dim=-1)  # (B, T, C+C_cond, H, W)\n",
    "        print(\"x_cond.shape\", x_cond.shape)\n",
    "        \n",
    "        x_cond = rearrange(x_cond, \"b t w h c -> b (t c) w h\")\n",
    "        print(\"x_cond reshaped\", x_cond.shape)\n",
    "        # Process through UNet\n",
    "        out_flat = self.unet(x_cond, mod=mod_for_unet)\n",
    "        print(\"out\",out_flat.shape)\n",
    "        # Reshape back to (B, T, C, H, W)\n",
    "        return out_flat.reshape(B, T, W, H, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_cast.decoders.channels_last import ChannelsLast\n",
    "from auto_cast.encoders.permute_concat import PermuteConcat\n",
    "from auto_cast.models.encoder_decoder import EncoderDecoder\n",
    "from auto_cast.models.encoder_processor_decoder import EncoderProcessorDecoder\n",
    "from auto_cast.processors.diffusion import DiffusionProcessor\n",
    "from azula.noise import CosineSchedule\n",
    "\n",
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "n_channels = batch.input_fields.shape[-1]\n",
    "# Create schedule\n",
    "schedule = CosineSchedule()\n",
    "\n",
    "backbone = TemporalUNetBackbone(\n",
    "    in_channels=(n_channels+128)*n_steps_input,          # 1\n",
    "    out_channels=n_channels,         # 1\n",
    "    cond_channels=0,        # 1\n",
    "    mod_features=128,\n",
    "    hid_channels=(16, 32, 64),\n",
    "    hid_blocks=(2, 2, 2),\n",
    "    spatial=2,\n",
    "    periodic=False,\n",
    ")\n",
    "\n",
    "\n",
    "processor = DiffusionProcessor(\n",
    "    backbone=backbone,\n",
    "    schedule=schedule,\n",
    "    denoiser_type='karras',\n",
    "    learning_rate=1e-4,\n",
    "    n_steps_output=n_steps_output,  # 4\n",
    "    stride=1,\n",
    "    max_rollout_steps=10,\n",
    "    teacher_forcing_ratio=0.0,\n",
    ")\n",
    "encoder = IdentityEncoder()\n",
    "decoder = IdentityDecoder()\n",
    "\n",
    "model = EncoderProcessorDecoder.from_encoder_processor_decoder(\n",
    "    encoder_decoder=EncoderDecoder.from_encoder_decoder(\n",
    "        encoder=encoder, decoder=decoder\n",
    "    ),\n",
    "    processor=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_out.shape torch.Size([16, 4, 50, 50, 1])\n",
      "cond.shape torch.Size([16, 1, 50, 50, 1])\n",
      "torch.Size([16, 128])\n",
      "t_emb.shape torch.Size([16, 1, 50, 50, 128])\n",
      "x_cond.shape torch.Size([16, 1, 50, 50, 129])\n",
      "x_cond reshaped torch.Size([16, 129, 50, 50])\n",
      "out torch.Size([16, 1, 50, 50])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[16, 4, 50, 50, 1]' is invalid for input of size 40000",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<@beartype(auto_cast.models.encoder_processor_decoder.EncoderProcessorDecoder.__call__) at 0x320afea20>:33\u001b[39m, in \u001b[36m__call__\u001b[39m\u001b[34m(__beartype_object_13030348032, __beartype_get_violation, __beartype_conf, __beartype_object_13030315840, __beartype_check_meta, __beartype_func, *args, **kwargs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/auto-cast/src/auto_cast/models/encoder_processor_decoder.py:53\u001b[39m, in \u001b[36mEncoderProcessorDecoder.__call__\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: Batch) -> TensorBTSPlusC:\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/auto-cast/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/auto-cast/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<@beartype(auto_cast.processors.diffusion.DiffusionProcessor.forward) at 0x320a54cc0>:32\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(__beartype_object_4655406080, __beartype_get_violation, __beartype_conf, __beartype_check_meta, __beartype_func, *args, **kwargs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/auto-cast/src/auto_cast/processors/diffusion.py:70\u001b[39m, in \u001b[36mDiffusionProcessor.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<@beartype(auto_cast.processors.diffusion.DiffusionProcessor.map) at 0x320a54c20>:32\u001b[39m, in \u001b[36mmap\u001b[39m\u001b[34m(__beartype_object_4655406080, __beartype_get_violation, __beartype_conf, __beartype_check_meta, __beartype_func, *args, **kwargs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/auto-cast/src/auto_cast/processors/diffusion.py:67\u001b[39m, in \u001b[36mDiffusionProcessor.map\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     65\u001b[39m B, _, C, H, W = x.shape\n\u001b[32m     66\u001b[39m x_t = torch.randn(B, \u001b[38;5;28mself\u001b[39m.n_steps_output, C, H, W, device=x.device)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_denoise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<@beartype(auto_cast.processors.diffusion.DiffusionProcessor._denoise) at 0x320a57880>:68\u001b[39m, in \u001b[36m_denoise\u001b[39m\u001b[34m(__beartype_object_4655406080, __beartype_get_violation, __beartype_conf, __beartype_check_meta, __beartype_func, *args, **kwargs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/auto-cast/src/auto_cast/processors/diffusion.py:73\u001b[39m, in \u001b[36mDiffusionProcessor._denoise\u001b[39m\u001b[34m(self, x, t, cond)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_denoise\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, t: Tensor, cond: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     posterior = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdenoiser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m posterior.mean\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/auto-cast/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/auto-cast/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/auto-cast/.venv/lib/python3.12/site-packages/azula/denoise.py:316\u001b[39m, in \u001b[36mKarrasDenoiser.forward\u001b[39m\u001b[34m(self, x_t, t, **kwargs)\u001b[39m\n\u001b[32m    312\u001b[39m c_time = torch.log(sigma_t / alpha_t).reshape_as(t)\n\u001b[32m    314\u001b[39m dtype = get_module_dtype(\u001b[38;5;28mself\u001b[39m.backbone)\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_in\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43mc_time\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.to(x_t)\n\u001b[32m    322\u001b[39m mean = c_skip * x_t + c_out * output\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DiracPosterior(mean=mean)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/auto-cast/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/auto-cast/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mTemporalUNetBackbone.forward\u001b[39m\u001b[34m(self, x_out, t, cond)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mout\u001b[39m\u001b[33m\"\u001b[39m,out_flat.shape)\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Reshape back to (B, T, C, H, W)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mout_flat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[16, 4, 50, 50, 1]' is invalid for input of size 40000"
     ]
    }
   ],
   "source": [
    "model(batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Run trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "device = \"mps\"  # \"cpu\"\n",
    "# device = \"cpu\"\n",
    "trainer = L.Trainer(max_epochs=1, accelerator=device, log_every_n_steps=10)\n",
    "trainer.fit(model, datamodule.train_dataloader(), datamodule.val_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Run the evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, datamodule.test_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Example rollout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single element is the full trajectory\n",
    "batch = next(iter(datamodule.rollout_test_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First n_steps_input are inputs\n",
    "print(batch.input_fields.shape)\n",
    "# Remaining n_steps_output are outputs\n",
    "print(batch.output_fields.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rollout on one trajectory\n",
    "preds, trues = model.rollout(batch, free_running_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert trues is not None\n",
    "print(trues.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-cast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
